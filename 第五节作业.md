1. 创建环境

   ```cmd
   studio-conda -t lmdeploy -o pytorch-2.1.2
   ```

   ![image-20240410111518630](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410111518630.png)

2. 安装LMDeploy

   ```cmd
   # 1. 激活虚拟环境。
   conda activate lmdeploy
   # 2. 安装lmdeploy
   pip install lmdeploy[all]==0.3.0
   ```

   ![image-20240410111929364](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410111929364.png)

3. 下载模型

   ```cmd
   # 1. 下载模型
   ls /root/share/new_models/Shanghai_AI_Laboratory/
   # 2. 拷贝模型
   ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/
   ```

   ![image-20240410112019815](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410112019815.png)

4. 运行模型

   ```cmd
   # 1. 创建模型运行文件
   touch /root/pipeline_transformer.py
   # 2. 写入运行代码 （见下方python代码）
   # 3. 运行代码（精度采用float16）
   python /root/pipeline_transformer.py
   ```

   写入运行代码:

   ```python
   import torch
   from transformers import AutoTokenizer, AutoModelForCausalLM
   
   tokenizer = AutoTokenizer.from_pretrained("/root/internlm2-chat-1_8b", trust_remote_code=True)
   
   # Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and cause OOM Error.
   model = AutoModelForCausalLM.from_pretrained("/root/internlm2-chat-1_8b", torch_dtype=torch.float16, trust_remote_code=True).cuda()
   model = model.eval()
   
   inp = "hello"
   print("[INPUT]", inp)
   response, history = model.chat(tokenizer, inp, history=[])
   print("[OUTPUT]", response)
   
   inp = "please provide three suggestions about time management"
   print("[INPUT]", inp)
   response, history = model.chat(tokenizer, inp, history=history)
   print("[OUTPUT]", response)
   
   ```

   ![image-20240410112102025](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410112102025.png)

5. 模型对话

   ```cmd
   # 对话模型采用 1.8b模型
   lmdeploy chat /root/internlm2-chat-1_8b
   ```

   ![image-20240410114426600](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410114426600.png)

6. 模型量化(lite)

   模型量化(lite)主要包括 KV8量化和W4A16量化。

   量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。

   1. Cache缓存大小设置

   ```cmd
   # 1. 不进行量化
   lmdeploy chat /root/internlm2-chat-1_8b
   # 2. 量化比0.5
   lmdeploy chat /root/internlm2-chat-1_8b --cache-max-entry-count 0.5
   # 3. 禁用缓存
   lmdeploy chat /root/internlm2-chat-1_8b --cache-max-entry-count 0.01
   ```

     **量化比0.5**：

   ![image-20240410114713044](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410114713044.png)

   ![image-20240410114749318](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410114749318.png)

   **禁用缓存**:

   ![image-20240410115434456](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410115434456.png)

   1. W4A16量化

   ```cmd
   # 1. 安装依赖库
   pip install einops==0.7.0
   # 2. 启动量化（w-bits 4）
   lmdeploy lite auto_awq \
      /root/internlm2-chat-1_8b \
     --calib-dataset 'ptb' \
     --calib-samples 128 \
     --calib-seqlen 1024 \
     --w-bits 4 \
     --w-group-size 128 \
     --work-dir /root/internlm2-chat-1_8b-4bit
   # 3. 启动量化后的模型
   lmdeploy chat /root/internlm2-chat-1_8b-4bit --model-format awq
   # 4. 降低缓存空间占比
   lmdeploy chat /root/internlm2-chat-1_8b-4bit --model-format awq --cache-max-entry-count 0.01
   ```

   **量化模型**：

   ![image-20240410120110463](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120110463.png)

   ​	![image-20240410120403831](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120403831.png)

​	**运行量化模型**：

​		![image-20240410120513213](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120513213.png)

​		![image-20240410120523933](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120523933.png)

​	**降低缓存空间占比**：

​		![image-20240410120613716](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120613716.png)

​		![image-20240410120623892](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240410120623892.png)