## 1. 配置开发环境

 1. 创建conda 环境

    ```cmd
    # 从本地 clone 一个已有 pytorch 的环境
    studio-conda xtuner0.1.17
    # 激活环境
    conda activate xtuner0.1.17
    ```

    ![image-20240415092724272](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240415092724272.png)

 2. 安装xtuner

    ```cmd
    # 拉取 0.1.17 的版本源码
    git clone -b v0.1.17  https://github.com/InternLM/xtuner
    # 进入源码目录
    cd /root/xtuner0117/xtuner
    # 从源码安装 XTuner
    pip install -e '.[all]'
    
    ```

    ![image-20240415100548441](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240415100548441.png)

## 2. 准备数据集

创建数据集存放文件夹

```cmd
# 前半部分是创建一个文件夹，后半部分是进入该文件夹。
mkdir -p /root/ft && cd /root/ft

# 在ft这个文件夹里再创建一个存放数据的data文件夹
mkdir -p /root/ft/data && cd /root/ft/data
```



数据集采用如下格式，共80条数据这边对数据进行重复处理

![image-20240415092905542](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240415092905542.png)

```python
import json

num = 100

with open("self_cognition_xuexi.json", "r", ) as fd:
    data = json.load(fd)
    print(data)
    all_data = num * data
    with open("self_cognition_xuexi_100.json", "w+") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)

```

![image-20240415100730062](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240415100730062.png)

## 3. 准备模型

```cmd
# 创建目标文件夹，确保它存在。
# -p选项意味着如果上级目录不存在也会一并创建，且如果目标文件夹已存在则不会报错。
mkdir -p /root/ft/model

# 复制内容到目标文件夹。-r选项表示递归复制整个文件夹。
cp -r /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/* /root/ft/model/
```

![image-20240415101000596](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240415101000596.png)

## 4. 准备配置文件

```cmd
# 列出所有内置配置文件
# xtuner list-cfg

# 假如我们想找到 internlm2-1.8b 模型里支持的配置文件
xtuner list-cfg -p internlm2_1_8b
# 创建一个存放 config 文件的文件夹
mkdir -p /root/ft/config

# 使用 XTuner 中的 copy-cfg 功能将 config 文件复制到指定的位置
xtuner copy-cfg internlm2_1_8b_qlora_alpaca_e3 /root/ft/config
```

配置文本修改后如下：

```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = '../model'
use_varlen_attn = False

# Data
alpaca_en_path = '../data/self_cognition_xuexi_100.json'
prompt_template = PROMPT_TEMPLATE.default
max_length = 2048
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 3
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 300
save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 300
SYSTEM = SYSTEM_TEMPLATE.alpaca
evaluation_inputs=[ '请你介绍一下你自己', '你是谁', '你是我的小助手吗']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path=alpaca_en_path),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=alpaca_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)

```

## 5. 模型开始训练

```cmd
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train
```

![image-20240416102809589](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416102809589.png)

###  1. 第一次验证

![image-20240416102850486](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416102850486.png)

### 2. 第二次验证

![image-20240416102915721](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416102915721.png)

### 3. 第三次验证

![image-20240416103433129](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416103433129.png)

### 4. 第四次验证

![image-20240416103458094](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416103458094.png)

可以看出 经过200个epoch 微调就已经具有明显效果

## 6. 训练结果

![image-20240416152314432](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416152314432.png)

我这边只保存了三个epoch



## 7. 模型转换

```cmd
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/epoch_3.pth /root/ft/huggingface
```

![image-20240416152538062](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416152538062.png)

![image-20240416152654866](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416152654866.png)

## 8. 模型整合

```cmd

mkdir -p /root/ft/final_model

export MKL_SERVICE_FORCE_INTEL=1

xtuner convert merge /root/ft/model /root/ft/huggingface /root/ft/final_model
```

## 9. 模型对话

```cmd
xtuner chat /root/ft/final_model --prompt-template internlm2_chat
```

![image-20240416153434445](https://raw.githubusercontent.com/kaisersama112/typora_image/master/image-20240416153434445.png)